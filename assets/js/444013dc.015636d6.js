"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[680],{6934:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>p,contentTitle:()=>o,default:()=>c,frontMatter:()=>r,metadata:()=>s,toc:()=>l});var a=n(7462),i=(n(7294),n(3905));n(1839);const r={id:"people-detection",title:"People Detection"},o=void 0,s={unversionedId:"examples/people_detection/people-detection",id:"examples/people_detection/people-detection",title:"People Detection",description:"Link to peopledetection/main.py",source:"@site/docs/examples/people_detection/people_detection.md",sourceDirName:"examples/people_detection",slug:"/examples/people_detection/",permalink:"/docs/examples/people_detection/",draft:!1,editUrl:"https://github.com/farm-ng/amiga-dev-kit/tree/main/website/docs/examples/people_detection/people_detection.md",tags:[],version:"current",frontMatter:{id:"people-detection",title:"People Detection"},sidebar:"examples",previous:{title:"Camera Client",permalink:"/docs/examples/camera_client/"},next:{title:"00 - Introduction",permalink:"/docs/tutorials/introduction/tutorial-introduction"}},p={},l=[{value:"Link to <code>people_detection/main.py</code>",id:"link-to-people_detectionmainpy",level:3},{value:"1. Install the farm-ng Brain ADK package",id:"1-install-the-farm-ng-brain-adk-package",level:3},{value:"2. Setup",id:"2-setup",level:3},{value:"3. Install Dependencies",id:"3-install-dependencies",level:3},{value:"3. Download the model data",id:"3-download-the-model-data",level:3},{value:"4. Execute the Python script",id:"4-execute-the-python-script",level:3},{value:"5. Code overview",id:"5-code-overview",level:3}],m={toc:l};function c(e){let{components:t,...n}=e;return(0,i.kt)("wrapper",(0,a.Z)({},m,n,{components:t,mdxType:"MDXLayout"}),(0,i.kt)("h3",{id:"link-to-people_detectionmainpy"},(0,i.kt)("a",{parentName:"h3",href:"https://github.com/farm-ng/farm-ng-amiga/blob/main/py/examples/people_detection/main.py"},"Link to ",(0,i.kt)("inlineCode",{parentName:"a"},"people_detection/main.py"))),(0,i.kt)("p",null,"This example shows how to use the ",(0,i.kt)("inlineCode",{parentName:"p"},"farm-ng-amiga")," library to detect people in a video stream."),(0,i.kt)("p",null,"It also shows how to implement a service and client via grpc."),(0,i.kt)("p",null,"The requirements to run this example are to have a ",(0,i.kt)("a",{parentName:"p",href:"/docs/brain/"},(0,i.kt)("strong",{parentName:"a"},"farm-ng brain"))," running Oak cameras and that your PC is on the same local network as the brain."),(0,i.kt)("p",null,"For testing you can use your webcam as a replacement, which we will go over later in this tutorial."),(0,i.kt)("h3",{id:"1-install-the-farm-ng-brain-adk-package"},"1. Install the ",(0,i.kt)("a",{parentName:"h3",href:"/docs/brain/brain-install"},"farm-ng Brain ADK package")),(0,i.kt)("h3",{id:"2-setup"},"2. Setup"),(0,i.kt)("admonition",{type:"tip"},(0,i.kt)("p",{parentName:"admonition"},"It is recommended to also install these dependencies and run the example in the brain ADK virtual environment.")),(0,i.kt)("p",null,"Create first a virtual environment"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"python3 -m venv venv\nsource venv/bin/activate\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"#assuming you have the farm-ng-amiga repository.\ncd farm-ng-amiga/py/examples/people_detection\n")),(0,i.kt)("h3",{id:"3-install-dependencies"},"3. Install Dependencies"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"pip install -r requirements.txt\n")),(0,i.kt)("h3",{id:"3-download-the-model-data"},"3. Download the model data"),(0,i.kt)("p",null,"In this example we use MobileNet SSD from tensorflow to be implemented in opencv."),(0,i.kt)("p",null,"Download the model weights and architecture:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"mkdir models\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"wget https://github.com/rdeepc/ExploreOpencvDnn/raw/master/models/frozen_inference_graph.pb -O models/frozen_inference_graph.pb\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"wget https://github.com/rdeepc/ExploreOpencvDnn/raw/master/models/ssd_mobilenet_v2_coco_2018_03_29.pbtxt -O models/ssd_mobilenet_v2_coco_2018_03_29.pbtxt\n")),(0,i.kt)("h3",{id:"4-execute-the-python-script"},"4. Execute the Python script"),(0,i.kt)("p",null,"Open one terminal or in that same terminal run the service:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"python service.py --port 50090 --models-dir models/\n# INFO:__main__:Loaded model: /home/edgar/software/farm-ng-amiga/py/examples/people_detection/models\n# INFO:__main__:Starting server on port 50090\n# INFO:__main__:Server started\n")),(0,i.kt)("p",null,"In another terminal, run the a pipeline using the client:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"python main.py --port-camera 50051 --port-detector 50090\n")),(0,i.kt)("p",null,"And you should see a window with the video stream and the detected people."),(0,i.kt)("h3",{id:"5-code-overview"},"5. Code overview"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},'python3 main.py --help\n# Copyright (c) farm-ng, inc.\n#\n# Licensed under the Amiga Development Kit License (the "License");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://github.com/farm-ng/amiga-dev-kit/blob/main/LICENSE\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an "AS IS" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport argparse\nimport asyncio\nfrom typing import List\n\nimport cv2\nimport numpy as np\nfrom client import PeopleDetectorClient\nfrom farm_ng.oak import oak_pb2\nfrom farm_ng.oak.camera_client import OakCameraClient\nfrom farm_ng.people_detection import people_detection_pb2\nfrom farm_ng.service.service_client import ClientConfig\nfrom limbus.core import Component\nfrom limbus.core import ComponentState\nfrom limbus.core import InputParams\nfrom limbus.core import OutputParams\nfrom limbus.core.pipeline import Pipeline\n\n\nclass AmigaCamera(Component):\n    def __init__(self, name: str, config: ClientConfig, stream_every_n: int) -> None:\n        super().__init__(name)\n        # configure the camera client\n        self.client = OakCameraClient(config)\n\n        # create a stream\n        self.stream = self.client.stream_frames(every_n=stream_every_n)\n\n    @staticmethod\n    def register_outputs(outputs: OutputParams) -> None:\n        outputs.declare("image", np.ndarray)\n\n    def _decode_image(self, image_data: bytes) -> np.ndarray:\n        image: np.ndarray = np.frombuffer(image_data, dtype="uint8")\n        image = cv2.imdecode(image, cv2.IMREAD_UNCHANGED)\n        return image\n\n    async def forward(self) -> ComponentState:\n        response = await self.stream.read()\n        frame: oak_pb2.OakSyncFrame = response.frame\n\n        await self.outputs.image.send(self._decode_image(frame.left.image_data))\n\n        return ComponentState.OK\n\n\nclass OpenCvCamera(Component):\n    def __init__(self, name: str) -> None:\n        super().__init__(name)\n        # configure the camera client\n        self.grabber = cv2.VideoCapture(0)\n\n    @staticmethod\n    def register_outputs(outputs: OutputParams) -> None:\n        outputs.declare("image", np.ndarray)\n\n    async def forward(self) -> ComponentState:\n        ret, frame = self.grabber.read()\n        if not ret:\n            return ComponentState.STOPPED\n\n        await self.outputs.image.send(frame)\n\n        return ComponentState.OK\n\n\nclass PeopleDetector(Component):\n    def __init__(self, name: str, config: ClientConfig, confidence_threshold: float) -> None:\n        super().__init__(name)\n        self.confidence_threshold = confidence_threshold\n        self.detector_client = PeopleDetectorClient(config)\n\n    @staticmethod\n    def register_inputs(inputs: InputParams) -> None:\n        inputs.declare("image", np.ndarray)\n\n    @staticmethod\n    def register_outputs(outputs: OutputParams) -> None:\n        outputs.declare("detections", List[people_detection_pb2.Detection])\n\n    async def forward(self) -> ComponentState:\n        # get the image\n        image: np.ndarray = await self.inputs.image.receive()\n\n        # send data to the server\n        detections: List[people_detection_pb2.Detection] = await self.detector_client.detect_people(\n            image, self.confidence_threshold\n        )\n\n        # send the detections\n        await self.outputs.detections.send(detections)\n        return ComponentState.OK\n\n\nclass Visualization(Component):\n    @staticmethod\n    def register_inputs(inputs: InputParams) -> None:\n        inputs.declare("image", np.ndarray)\n        inputs.declare("detections", List[people_detection_pb2.Detection])\n\n    async def forward(self) -> ComponentState:\n        image, detections = await asyncio.gather(self.inputs.image.receive(), self.inputs.detections.receive())\n\n        image_vis = image.copy()\n        for det in detections:\n            image_vis = cv2.rectangle(\n                image_vis, (int(det.x), int(det.y)), (int(det.x + det.width), int(det.y + det.height)), (0, 255, 0), 2\n            )\n\n        cv2.namedWindow("image", cv2.WINDOW_NORMAL)\n        cv2.imshow("image", image_vis)\n        cv2.waitKey(1)\n\n\nasync def main(config_camera: ClientConfig, config_detector: ClientConfig) -> None:\n\n    cam = AmigaCamera("amiga-camera", config_camera, stream_every_n=config_camera.stream_every_n)\n    # NOTE: use the OpenCvCamera if you want to use a webcam\n    # cam = OpenCvCamera("opencv-camera")\n    detector = PeopleDetector("people-detector", config_detector, confidence_threshold=0.5)\n    viz = Visualization("visualization")\n\n    cam.outputs.image >> detector.inputs.image\n    cam.outputs.image >> viz.inputs.image\n    detector.outputs.detections >> viz.inputs.detections\n\n    pipeline = Pipeline()\n    pipeline.add_nodes([cam, detector, viz])\n\n    await pipeline.async_run()\n\n\nif __name__ == "__main__":\n    parser = argparse.ArgumentParser(prog="amiga-people-detector")\n    parser.add_argument("--port-camera", type=int, required=True, help="The camera port.")\n    parser.add_argument("--address-camera", type=str, default="localhost", help="The camera address")\n    parser.add_argument("--port-detector", type=int, required=True, help="The camera port.")\n    parser.add_argument("--address-detector", type=str, default="localhost", help="The camera address")\n    parser.add_argument("--stream-every-n", type=int, default=5, help="Streaming frequency")\n    args = parser.parse_args()\n\n    # create the config for the clients\n    config_camera = ClientConfig(port=args.port_camera, address=args.address_camera)\n    config_camera.stream_every_n = args.stream_every_n\n\n    config_detector = ClientConfig(port=args.port_detector, address=args.address_detector)\n\n    # run the main\n    asyncio.run(main(config_camera, config_detector))\n')),(0,i.kt)("admonition",{type:"tip"},(0,i.kt)("p",{parentName:"admonition"},"We highgly recommend to have some basic knowledge about ",(0,i.kt)("a",{parentName:"p",href:"https://docs.python.org/3/library/asyncio.html"},(0,i.kt)("strong",{parentName:"a"},(0,i.kt)("inlineCode",{parentName:"strong"},"asyncio"))),".")))}c.isMDXComponent=!0}}]);