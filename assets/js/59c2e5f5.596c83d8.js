"use strict";(self.webpackChunkamiga_developers_website=self.webpackChunkamiga_developers_website||[]).push([[4467],{6951:(e,r,a)=>{a.r(r),a.d(r,{assets:()=>s,contentTitle:()=>c,default:()=>h,frontMatter:()=>i,metadata:()=>o,toc:()=>l});var n=a(4848),t=a(8453);const i={id:"camera-aruco-detector",title:"Camera Aruco Detector"},c=void 0,o={id:"examples/camera_aruco_detector/camera-aruco-detector",title:"Camera Aruco Detector",description:"Before diving into this code, here's a quick heads-up on what you'll need to be familiar with:",source:"@site/docs/examples/camera_aruco_detector/README.md",sourceDirName:"examples/camera_aruco_detector",slug:"/examples/camera_aruco_detector/",permalink:"/docs/examples/camera_aruco_detector/",draft:!1,unlisted:!1,editUrl:"https://github.com/farm-ng/amiga-dev-kit/tree/main/website/docs/examples/camera_aruco_detector/README.md",tags:[],version:"current",frontMatter:{id:"camera-aruco-detector",title:"Camera Aruco Detector"},sidebar:"examples",previous:{title:"Camera Calibration",permalink:"/docs/examples/camera_calibration/"},next:{title:"Camera Pointcloud",permalink:"/docs/examples/camera_pointcloud/"}},s={},l=[{value:"1. Install the farm-ng Brain ADK package",id:"1-install-the-farm-ng-brain-adk-package",level:3},{value:"2. Install the example&#39;s dependencies",id:"2-install-the-examples-dependencies",level:3},{value:"3. The ArUco detector",id:"3-the-aruco-detector",level:3},{value:"4. The main function",id:"4-the-main-function",level:3},{value:"5. Execute the Python script",id:"5-execute-the-python-script",level:3}];function d(e){const r={a:"a",admonition:"admonition",code:"code",h3:"h3",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsxs)(r.admonition,{title:"Basic Knowledge Requirements",type:"info",children:[(0,n.jsx)(r.p,{children:"Before diving into this code, here's a quick heads-up on what you'll need to be familiar with:"}),(0,n.jsxs)(r.ol,{children:["\n",(0,n.jsxs)(r.li,{children:[(0,n.jsx)(r.strong,{children:"Python Programming"}),": It's important to have a good grasp of Python, especially with concepts\nlike ",(0,n.jsx)(r.code,{children:"functions"}),", ",(0,n.jsx)(r.code,{children:"loops"}),", and ",(0,n.jsx)(r.code,{children:"classes"}),", since the example utilizes these fundamentals."]}),"\n",(0,n.jsxs)(r.li,{children:[(0,n.jsx)(r.strong,{children:"Asynchronous Programming with asyncio"}),": Familiarity with Python's asyncio for writing concurrent\ncode using the ",(0,n.jsx)(r.code,{children:"async/await"})," syntax."]}),"\n",(0,n.jsxs)(r.li,{children:[(0,n.jsx)(r.strong,{children:"Understanding of ArUco Markers"}),": Understand what ArUco markers are and how they function.\nThese markers are used for various applications in computer vision,\nmost commonly for camera calibration and pose estimation,\nand this example specifically utilizes them for detecting their positions in image frames."]}),"\n",(0,n.jsxs)(r.li,{children:[(0,n.jsx)(r.strong,{children:"Familiarity with OpenCV's ArUco Library"}),": The example uses OpenCV's functionalities to handle ArUco\nmarkers, including detection and pose estimation.\nKnowledge of functions like ",(0,n.jsx)(r.code,{children:"detectMarkers"}),", ",(0,n.jsx)(r.code,{children:"drawDetectedMarkers"}),", and ",(0,n.jsx)(r.code,{children:"estimatePoseSingleMarkers"}),"\nis important."]}),"\n",(0,n.jsxs)(r.li,{children:[(0,n.jsx)(r.strong,{children:"Camera Calibration and 3D Geometry Concepts"}),": Knowledge of camera intrinsic parameters\nto compute the camera matrix and distortion coefficients.\nKnowing how these parameters influence the projection of 3D points onto a 2D image is important."]}),"\n",(0,n.jsxs)(r.li,{children:[(0,n.jsx)(r.strong,{children:"Experience with Image Processing and Decoding"}),": Users should be comfortable with image manipulation\ntasks, as the example involves converting images to grayscale for marker."]}),"\n",(0,n.jsxs)(r.li,{children:[(0,n.jsx)(r.strong,{children:(0,n.jsx)(r.a,{href:"/docs/concepts/oak_service/",children:"farm-ng Oak Service Overview"})}),":\nThis overview provides a base understanding of the gRPC service the client you create will connect to."]}),"\n"]})]}),"\n",(0,n.jsxs)(r.p,{children:["In the ",(0,n.jsx)(r.a,{href:"https://github.com/farm-ng/farm-ng-amiga/blob/main/py/examples/camera_aruco_detector/main.py",children:(0,n.jsx)(r.strong,{children:"Camera ArUco"})}),"\nexample we are going show how to implement a simple ArUco marker detector\nusing the ",(0,n.jsx)(r.a,{href:"https://opencv.org/",children:(0,n.jsx)(r.strong,{children:"OpenCV"})})," library and the Camera service to\ndetect and estimate the 6DoF pose of an ArUco marker useful for robot localization."]}),"\n",(0,n.jsxs)(r.p,{children:["To understand the basics of the ArUco marker detection and pose estimation, please\nrefer to the ",(0,n.jsx)(r.a,{href:"https://docs.opencv.org/master/d5/dae/tutorial_aruco_detection.html",children:(0,n.jsx)(r.strong,{children:"OpenCV ArUco tutorial"})}),"."]}),"\n",(0,n.jsxs)(r.p,{children:["We also recommend to read first the\n",(0,n.jsx)(r.a,{href:"/docs/examples/camera_client",children:(0,n.jsx)(r.strong,{children:(0,n.jsx)(r.code,{children:"Camera Client"})})})," and\n",(0,n.jsx)(r.a,{href:"/docs/examples/camera_calibration",children:(0,n.jsx)(r.strong,{children:(0,n.jsx)(r.code,{children:"Camera Calibration"})})})," tutorials."]}),"\n",(0,n.jsxs)(r.p,{children:["To successfully run this example, you must use your local PC, as the example won't\nwork if executed directly from a brain (because of the popup window).\nEnsure that a ",(0,n.jsx)(r.a,{href:"/docs/brain/",children:(0,n.jsx)(r.strong,{children:"farm-ng brain"})})," running Oak cameras is active.\nYour local PC should be either connected to the same local network as the brain\nor linked to it through ",(0,n.jsxs)(r.a,{href:"https://tailscale.com/",children:[(0,n.jsx)(r.strong,{children:"tailscale"}),"."]})]}),"\n",(0,n.jsxs)(r.h3,{id:"1-install-the-farm-ng-brain-adk-package",children:["1. Install the ",(0,n.jsx)(r.a,{href:"/docs/brain/brain-install",children:"farm-ng Brain ADK package"})]}),"\n",(0,n.jsx)(r.h3,{id:"2-install-the-examples-dependencies",children:"2. Install the example's dependencies"}),"\n",(0,n.jsx)(r.admonition,{type:"tip",children:(0,n.jsx)(r.p,{children:"It is recommended to also install these dependencies and run the\nexample in the brain ADK virtual environment."})}),"\n",(0,n.jsx)(r.pre,{children:(0,n.jsx)(r.code,{className:"language-bash",children:"# assuming you're already in the amiga-dev-kit/ directory\ncd farm-ng-amiga/py/examples/camera_aruco_detector\n"})}),"\n",(0,n.jsx)(r.pre,{children:(0,n.jsx)(r.code,{className:"language-bash",children:"pip3 install -r requirements.txt\n"})}),"\n",(0,n.jsx)(r.p,{children:"This is will install the following dependencies:"}),"\n",(0,n.jsxs)(r.ul,{children:["\n",(0,n.jsxs)(r.li,{children:[(0,n.jsx)(r.code,{children:"farm-ng-amiga"})," - the farm-ng python client library"]}),"\n",(0,n.jsxs)(r.li,{children:[(0,n.jsx)(r.code,{children:"opencv-python"})," - the OpenCV python library"]}),"\n",(0,n.jsxs)(r.li,{children:[(0,n.jsx)(r.code,{children:"opencv-contrib-python"})," - the OpenCV python library with extra modules (needed for ArUco detection)"]}),"\n"]}),"\n",(0,n.jsx)(r.h3,{id:"3-the-aruco-detector",children:"3. The ArUco detector"}),"\n",(0,n.jsxs)(r.p,{children:["In order to detect the ArUco markers, we need to create an ArUco detector object and\nconfigure it with the desired parameters. To do this, we'll create an auxiliary class\n",(0,n.jsx)(r.code,{children:"ArucoDetector"})," that will hold the detector object and the configuration parameters."]}),"\n",(0,n.jsxs)(r.p,{children:["In this example, we'll use the ",(0,n.jsx)(r.code,{children:"DICT_6X6_250"})," ArUco dictionary type and a marker size of 0.1 meters\nto detect the ArUco markers following the ",(0,n.jsx)(r.a,{href:"https://docs.opencv.org/master/d5/dae/tutorial_aruco_detection.html",children:(0,n.jsx)(r.strong,{children:"OpenCV ArUco tutorial"})}),"."]}),"\n",(0,n.jsx)(r.pre,{children:(0,n.jsx)(r.code,{className:"language-python",children:'class ArucoDetector:\n    """A class for detecting ArUco markers in an image frame."""\n\n    def __init__(self, aruco_dict_type: str, marker_size: float) -> None:\n        """Initialize the ArUco detector.\n\n        Args:\n            aruco_dict_type (str): The ArUco dictionary type.\n            marker_size (float): The size of the ArUco marker in meters.\n        """\n        self._detector = self._create_detector(aruco_dict_type)\n        self._marker_size = marker_size\n\n    def _create_detector(self, aruco_dict_type: str) -> cv2.aruco.ArucoDetector:\n        """Create an ArUco detector.\n\n        Args:\n            aruco_dict_type (str): The ArUco dictionary type.\n\n        Returns:\n            cv2.aruco.ArucoDetector: The ArUco detector.\n        """\n        aruco_params = cv2.aruco.DetectorParameters()\n\n        # See all the available ArUco dictionary types here:\n        # https://docs.opencv.org/4.x/de/d67/group__objdetect__aruco.html#ga4e13135a118f497c6172311d601ce00d\n        aruco_dict = cv2.aruco.getPredefinedDictionary(getattr(cv2.aruco, aruco_dict_type))\n        return cv2.aruco.ArucoDetector(aruco_dict, aruco_params)\n'})}),"\n",(0,n.jsx)(r.p,{children:"We will also create a couple of internal utilities to create the reference 3d points needed by the\npose estimation solver."}),"\n",(0,n.jsx)(r.pre,{children:(0,n.jsx)(r.code,{className:"language-python",children:'    def _create_object_points(self, marker_size: float) -> np.ndarray:\n        """Create the object points for the ArUco markers.\n\n        Args:\n            marker_size (float): The size of the ArUco marker in meters.\n\n        Returns:\n            np.ndarray: The object points for the ArUco markers.\n        """\n        size_half: float = marker_size / 2.0\n        return np.array(\n            [\n                [-size_half, -size_half, 0],\n                [size_half, -size_half, 0],\n                [size_half, size_half, 0],\n                [-size_half, size_half, 0],\n            ],\n            dtype=np.float32,\n        )\n'})}),"\n",(0,n.jsxs)(r.p,{children:["As described in the ",(0,n.jsx)(r.a,{href:"/docs/examples/camera_pointcloud",children:(0,n.jsx)(r.strong,{children:"Camera to Point Cloud"})}),"\ntutorial, we need to create a camera intrinsics matrix\nfrom the camera calibration parameters. We'll create a utility function to do this."]}),"\n",(0,n.jsx)(r.pre,{children:(0,n.jsx)(r.code,{className:"language-python",children:'@staticmethod\ndef get_camera_matrix(camera_data: oak_pb2.CameraData) -> np.ndarray:\n    """Compute the camera matrix from the camera calibration data.\n\n    Args:\n        camera_data (oak_pb2.CameraData): The camera calibration data.\n\n    Returns:\n        Tensor: The camera matrix with shape 3x3.\n    """\n    fx = camera_data.intrinsic_matrix[0]\n    fy = camera_data.intrinsic_matrix[4]\n    cx = camera_data.intrinsic_matrix[2]\n    cy = camera_data.intrinsic_matrix[5]\n\n    return np.array([[fx, 0, cx], [0, fy, cy], [0, 0, 1]])\n'})}),"\n",(0,n.jsx)(r.p,{children:"Finally, we'll create a method to detect the Aruco markers in an image frame and estimate their\npose that can be used later to build other applications. In this example, for educational purposes,\nwe will render the 3d detection into the original image."}),"\n",(0,n.jsx)(r.pre,{children:(0,n.jsx)(r.code,{className:"language-python",children:'def detect_pose(self, frame: np.ndarray, camera_matrix: np.ndarray, distortion_coeff: np.ndarray):\n    """Detect ArUco markers in an image frame.\n\n        Args:\n            frame (np.ndarray): The image frame in rgb format with shape HxWx3.\n            camera_matrix (np.ndarray): The camera matrix with shape 3x3.\n            distortion_coeff (np.ndarray): The distortion coefficients with shape 1x5.\n    """\n    assert len(frame.shape) == 3 and frame.shape[2] == 3, "image must be rgb"\n\n    # Convert the image to grayscale\n    gray = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n\n    # Detect the markers\n    corners, _, _ = self._detector.detectMarkers(gray)\n\n    print(f"Detected {len(corners)} markers")\n\n    rvec = []\n    tvec = []\n    frame_vis = frame\n\n    for corner in corners:\n        # Estimate the pose of the marker\n        _rvec, _tvec, _ = cv2.aruco.estimatePoseSingleMarkers(\n            corner, self._marker_size, camera_matrix, distortion_coeff\n        )\n\n        # sotre the results\n        rvec.append(_rvec)\n        tvec.append(_tvec)\n\n        # Draw the detected marker and its pose\n        frame_vis = cv2.drawFrameAxes(\n            frame, camera_matrix, distortion_coeff, _rvec, _tvec, self._marker_size * 0.5\n        )\n\n    # Draw the detected markers\n    frame_vis = cv2.aruco.drawDetectedMarkers(frame_vis, corners)\n\n    return (np.array(rvec), np.array(tvec)), frame_vis\n'})}),"\n",(0,n.jsx)(r.h3,{id:"4-the-main-function",children:"4. The main function"}),"\n",(0,n.jsx)(r.p,{children:"Now that we have the Aruco detector, we can create the main function from where we'll\nfirst request the camera calibration data and then subscribe to the camera service to receive\nthe camera stream and detect the Aruco markers. Finally, we'll visualize the detections in the\noriginal image using OpenCV."}),"\n",(0,n.jsx)(r.pre,{children:(0,n.jsx)(r.code,{className:"language-python",children:'# create a client to the camera service\nconfig: EventServiceConfig = proto_from_json_file(args.service_config, EventServiceConfig())\n\n# create the camera client\ncamera_client = EventClient(config)\n\n# request the camera calibration data\ncalibration: oak_pb2.OakCalibration = await camera_client.request_reply("/calibration", Empty(), decode=True)\n\n# create the ArUco detector\ndetector = ArucoDetector(aruco_dict_type=args.aruc_type, marker_size=args.marker_size)\n\n# NOTE: The OakCalibration message contains the camera calibration data for all the cameras.\n# Since we are interested in the disparity image, we will use the calibration data for the right camera\n# which is the first camera in the list.\ncamera_matrix: np.ndarray = detector.get_camera_matrix(calibration.camera_data[0])\ndistortion_coeff = np.array(calibration.camera_data[0].distortion_coeff)\n\nasync for event, message in camera_client.subscribe(config.subscriptions[0], decode=True):\n    # cast image data bytes to numpy and decode\n    image: np.ndarray = cv2.imdecode(np.frombuffer(message.image_data, dtype="uint8"), cv2.IMREAD_UNCHANGED)\n\n    # detect the aruco markers in the image\n    # NOTE: do something with the detections here, e.g. publish them to the event service\n    detections, image_vis = detector.detect_pose(image, camera_matrix, distortion_coeff)\n\n    # visualize the image\n    cv2.namedWindow("image", cv2.WINDOW_NORMAL)\n    cv2.imshow("image", image_vis)\n    cv2.waitKey(1)\n'})}),"\n",(0,n.jsx)(r.h3,{id:"5-execute-the-python-script",children:"5. Execute the Python script"}),"\n",(0,n.jsxs)(r.admonition,{type:"info",children:[(0,n.jsxs)(r.p,{children:["Since this example must be run from your local PC, you will need update the ",(0,n.jsx)(r.code,{children:"service_config.json"}),"\nby modifying the ",(0,n.jsx)(r.code,{children:"host"})," field with your Amiga brain name."]}),(0,n.jsxs)(r.p,{children:["Please check out ",(0,n.jsx)(r.a,{href:"/docs/concepts/system_overview/#running-examples-on-your-local-machine",children:"Amiga Development 101"}),"\nfor more details."]})]}),"\n",(0,n.jsxs)(r.p,{children:["You can also stream the stereo left or right images or the camera's disparity by changing\nthe ",(0,n.jsx)(r.code,{children:"path"})," field (e.g., /left) or change the port and service name to instead stream\nthe Oak1 camera."]}),"\n",(0,n.jsx)(r.p,{children:"In order to run the example, we need to provide the path to the camera service config file"}),"\n",(0,n.jsx)(r.pre,{children:(0,n.jsx)(r.code,{className:"language-bash",children:"python3 main.py --service-config service_config.json\n"})}),"\n",(0,n.jsx)(r.p,{children:"Optionally, we can also provide the Aruco dictionary type and the marker size"}),"\n",(0,n.jsx)(r.pre,{children:(0,n.jsx)(r.code,{className:"language-bash",children:"python3 main.py --service-config service_config.json --aruco-type DICT_6X6_250 --marker-size 0.1\n"})}),"\n",(0,n.jsx)(r.p,{children:"The example will open a window with the camera stream and the detected Aruco markers."}),"\n",(0,n.jsx)(r.p,{children:(0,n.jsx)(r.img,{src:"https://github.com/farm-ng/amiga-dev-kit/assets/5157099/a0a1abb4-4727-4c2e-be76-b94868dd75fa",alt:"Screenshot from 2023-10-06 16-34-59"})})]})}function h(e={}){const{wrapper:r}={...(0,t.R)(),...e.components};return r?(0,n.jsx)(r,{...e,children:(0,n.jsx)(d,{...e})}):d(e)}},8453:(e,r,a)=>{a.d(r,{R:()=>c,x:()=>o});var n=a(6540);const t={},i=n.createContext(t);function c(e){const r=n.useContext(i);return n.useMemo((function(){return"function"==typeof e?e(r):{...r,...e}}),[r,e])}function o(e){let r;return r=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:c(e.components),n.createElement(i.Provider,{value:r},e.children)}}}]);