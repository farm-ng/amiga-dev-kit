"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[4271],{7114:(e,a,t)=>{t.r(a),t.d(a,{assets:()=>s,contentTitle:()=>i,default:()=>d,frontMatter:()=>o,metadata:()=>c,toc:()=>m});var r=t(7462),n=(t(7294),t(3905));t(1839);const o={id:"camera-aruco-detector",title:"Camera Aruco Detector"},i=void 0,c={unversionedId:"examples/camera_aruco_detector/camera-aruco-detector",id:"examples/camera_aruco_detector/camera-aruco-detector",title:"Camera Aruco Detector",description:"Link to cameraarucodetector/main.py",source:"@site/docs/examples/camera_aruco_detector/README.md",sourceDirName:"examples/camera_aruco_detector",slug:"/examples/camera_aruco_detector/",permalink:"/docs/examples/camera_aruco_detector/",draft:!1,editUrl:"https://github.com/farm-ng/amiga-dev-kit/tree/main/website/docs/examples/camera_aruco_detector/README.md",tags:[],version:"current",frontMatter:{id:"camera-aruco-detector",title:"Camera Aruco Detector"},sidebar:"examples",previous:{title:"Events Recorder",permalink:"/docs/examples/events_recorder/"},next:{title:"Camera Client",permalink:"/docs/examples/camera_client/"}},s={},m=[{value:"Link to <code>camera_aruco_detector/main.py</code>",id:"link-to-camera_aruco_detectormainpy",level:3},{value:"1. Install the farm-ng Brain ADK package",id:"1-install-the-farm-ng-brain-adk-package",level:3},{value:"2. Install the example&#39;s dependencies",id:"2-install-the-examples-dependencies",level:3},{value:"3. The Aruco detector",id:"3-the-aruco-detector",level:3},{value:"4. The main function",id:"4-the-main-function",level:3},{value:"5. Execute the Python script",id:"5-execute-the-python-script",level:3}],l={toc:m};function d(e){let{components:a,...t}=e;return(0,n.kt)("wrapper",(0,r.Z)({},l,t,{components:a,mdxType:"MDXLayout"}),(0,n.kt)("h3",{id:"link-to-camera_aruco_detectormainpy"},(0,n.kt)("a",{parentName:"h3",href:"https://github.com/farm-ng/farm-ng-amiga/blob/main-v2/py/examples/camera_aruco_detector/main.py"},"Link to ",(0,n.kt)("inlineCode",{parentName:"a"},"camera_aruco_detector/main.py"))),(0,n.kt)("p",null,"In this example we are going show how to implement a simple Aruco marker detector\nusing the ",(0,n.kt)("a",{parentName:"p",href:"https://opencv.org/"},(0,n.kt)("strong",{parentName:"a"},"OpenCV"))," library and the Camera service to\ndetect and estimate the 6DoF pose of an Aruco marker useful for robot localization."),(0,n.kt)("p",null,"To understand the basics of the Aruco marker detection and pose estimation, please\nrefer to the ",(0,n.kt)("a",{parentName:"p",href:"https://docs.opencv.org/master/d5/dae/tutorial_aruco_detection.html"},(0,n.kt)("strong",{parentName:"a"},"OpenCV Aruco tutorial")),"."),(0,n.kt)("p",null,"We also recommend to read first the\n",(0,n.kt)("a",{parentName:"p",href:"/docs/examples/camera_client"},(0,n.kt)("strong",{parentName:"a"},(0,n.kt)("inlineCode",{parentName:"strong"},"Camera Client")))," and\n",(0,n.kt)("a",{parentName:"p",href:"/docs/examples/camera_calibration"},(0,n.kt)("strong",{parentName:"a"},(0,n.kt)("inlineCode",{parentName:"strong"},"Camera Calibration")))," tutorials."),(0,n.kt)("p",null,"To successfully run this example, you must use your local PC, as the example won't\nwork if executed directly from a brain (because of the popup window).\nEnsure that a ",(0,n.kt)("a",{parentName:"p",href:"/docs/brain/"},(0,n.kt)("strong",{parentName:"a"},"farm-ng brain"))," running Oak cameras is active.\nYour local PC should be either connected to the same local network as the brain\nor linked to it through tailscale."),(0,n.kt)("h3",{id:"1-install-the-farm-ng-brain-adk-package"},"1. Install the ",(0,n.kt)("a",{parentName:"h3",href:"/docs/brain/brain-install"},"farm-ng Brain ADK package")),(0,n.kt)("h3",{id:"2-install-the-examples-dependencies"},"2. Install the example's dependencies"),(0,n.kt)("admonition",{type:"tip"},(0,n.kt)("p",{parentName:"admonition"},"It is recommended to also install these dependencies and run the\nexample in the brain ADK virtual environment.")),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-bash"},"# assuming you're already in the amiga-dev-kit/ directory\ncd farm-ng-amiga/py/examples/camera_aruco_detector\n")),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-bash"},"pip3 install -r requirements.txt\n")),(0,n.kt)("p",null,"This is will install the following dependencies:"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre"},"- `farm-ng-amiga` - the farm-ng python client library\n- `opencv-python` - the OpenCV python library\n- `opencv-contrib-python` - the OpenCV python library with extra modules (needed for Aruco detection)\n")),(0,n.kt)("h3",{id:"3-the-aruco-detector"},"3. The Aruco detector"),(0,n.kt)("p",null,"In order to detect the Aruco markers, we need to create an Aruco detector object and\nconfigure it with the desired parameters. To do this, we'll create an auxiliary class\n",(0,n.kt)("inlineCode",{parentName:"p"},"ArucoDetector")," that will hold the detector object and the configuration parameters."),(0,n.kt)("p",null,"In this example, we'll use the ",(0,n.kt)("inlineCode",{parentName:"p"},"DICT_6X6_250")," Aruco dictionary type and a marker size of 0.1 meters\nto detect the Aruco markers following the ",(0,n.kt)("a",{parentName:"p",href:"https://docs.opencv.org/master/d5/dae/tutorial_aruco_detection.html"},(0,n.kt)("strong",{parentName:"a"},"OpenCV Aruco tutorial")),"."),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-python"},'class ArucoDetector:\n    """A class for detecting ArUco markers in an image frame."""\n\n    def __init__(self, aruco_dict_type: str, marker_size: float) -> None:\n        """Initialize the ArUco detector.\n\n        Args:\n            aruco_dict_type (str): The ArUco dictionary type.\n            marker_size (float): The size of the ArUco marker in meters.\n        """\n        self._detector = self._create_detector(aruco_dict_type)\n        self._marker_size = marker_size\n\n    def _create_detector(self, aruco_dict_type: str) -> cv2.aruco.ArucoDetector:\n        """Create an ArUco detector.\n\n        Args:\n            aruco_dict_type (str): The ArUco dictionary type.\n\n        Returns:\n            cv2.aruco.ArucoDetector: The ArUco detector.\n        """\n        aruco_params = cv2.aruco.DetectorParameters()\n\n        # See all the available ArUco dictionary types here:\n        # https://docs.opencv.org/4.x/de/d67/group__objdetect__aruco.html#ga4e13135a118f497c6172311d601ce00d\n        aruco_dict = cv2.aruco.getPredefinedDictionary(getattr(cv2.aruco, aruco_dict_type))\n        return cv2.aruco.ArucoDetector(aruco_dict, aruco_params)\n')),(0,n.kt)("p",null,"We will also create a couple of internal utilities to create the reference 3d points needed by the\npose estimation solver."),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-python"},'    def _create_object_points(self, marker_size: float) -> np.ndarray:\n        """Create the object points for the ArUco markers.\n\n        Args:\n            marker_size (float): The size of the ArUco marker in meters.\n\n        Returns:\n            np.ndarray: The object points for the ArUco markers.\n        """\n        size_half: float = marker_size / 2.0\n        return np.array(\n            [\n                [-size_half, -size_half, 0],\n                [size_half, -size_half, 0],\n                [size_half, size_half, 0],\n                [-size_half, size_half, 0],\n            ],\n            dtype=np.float32,\n        )\n')),(0,n.kt)("p",null,"As described in the ",(0,n.kt)("a",{parentName:"p",href:"/docs/examples/camera_pointcloud"},(0,n.kt)("strong",{parentName:"a"},"Camera to Pointcloud")),"\ntutorial, we need to create a camera intrinsics matrix\nfrom the camera calibration parameters. We'll create a utility function to do this."),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-python"},'@staticmethod\ndef get_camera_matrix(camera_data: oak_pb2.CameraData) -> np.ndarray:\n    """Compute the camera matrix from the camera calibration data.\n\n    Args:\n        camera_data (oak_pb2.CameraData): The camera calibration data.\n\n    Returns:\n        Tensor: The camera matrix with shape 3x3.\n    """\n    fx = camera_data.intrinsic_matrix[0]\n    fy = camera_data.intrinsic_matrix[4]\n    cx = camera_data.intrinsic_matrix[2]\n    cy = camera_data.intrinsic_matrix[5]\n\n    return np.array([[fx, 0, cx], [0, fy, cy], [0, 0, 1]])\n')),(0,n.kt)("p",null,"Finally, we'll create a method to detect the Aruco markers in an image frame and estimate their\npose that can be used later to build other applications. In this example, for educational purposes,\nwe will render the 3d detection into the original image."),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-python"},'def detect_pose(self, frame: np.ndarray, camera_matrix: np.ndarray, distortion_coeff: np.ndarray):\n    """Detect ArUco markers in an image frame.\n\n        Args:\n            frame (np.ndarray): The image frame in rgb format with shape HxWx3.\n            camera_matrix (np.ndarray): The camera matrix with shape 3x3.\n            distortion_coeff (np.ndarray): The distortion coefficients with shape 1x5.\n    """\n    assert len(frame.shape) == 3 and frame.shape[2] == 3, "image must be rgb"\n\n    # Convert the image to grayscale\n    gray = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n\n    # Detect the markers\n    corners, _, _ = self._detector.detectMarkers(gray)\n\n    print(f"Detected {len(corners)} markers")\n\n    rvec = []\n    tvec = []\n    frame_vis = frame\n\n    for corner in corners:\n        # Estimate the pose of the marker\n        _rvec, _tvec, _ = cv2.aruco.estimatePoseSingleMarkers(\n            corner, self._marker_size, camera_matrix, distortion_coeff\n        )\n\n        # sotre the results\n        rvec.append(_rvec)\n        tvec.append(_tvec)\n\n        # Draw the detected marker and its pose\n        frame_vis = cv2.drawFrameAxes(\n            frame, camera_matrix, distortion_coeff, _rvec, _tvec, self._marker_size * 0.5\n        )\n\n    # Draw the detected markers\n    frame_vis = cv2.aruco.drawDetectedMarkers(frame_vis, corners)\n\n    return (np.array(rvec), np.array(tvec)), frame_vis\n')),(0,n.kt)("h3",{id:"4-the-main-function"},"4. The main function"),(0,n.kt)("p",null,"Now that we have the Aruco detector, we can create the main function from where we'll\nfirst request the camera calibration data and then subscribe to the camera service to receive\nthe camera stream and detect the Aruco markers. Finally, we'll visualize the detections in the\noriginal image using OpenCV."),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-python"},'# create a client to the camera service\nconfig: EventServiceConfig = proto_from_json_file(args.service_config, EventServiceConfig())\n\n# create the camera client\ncamera_client = EventClient(config)\n\n# request the camera calibration data\ncalibration: oak_pb2.OakCalibration = await camera_client.request_reply("/calibration", Empty(), decode=True)\n\n# create the ArUco detector\ndetector = ArucoDetector(aruco_dict_type=args.aruc_type, marker_size=args.marker_size)\n\n# NOTE: The OakCalibration message contains the camera calibration data for all the cameras.\n# Since we are interested in the disparity image, we will use the calibration data for the right camera\n# which is the first camera in the list.\ncamera_matrix: np.ndarray = detector.get_camera_matrix(calibration.camera_data[0])\ndistortion_coeff = np.array(calibration.camera_data[0].distortion_coeff)\n\nasync for event, message in camera_client.subscribe(config.subscriptions[0], decode=True):\n    # cast image data bytes to numpy and decode\n    image: np.ndarray = cv2.imdecode(np.frombuffer(message.image_data, dtype="uint8"), cv2.IMREAD_UNCHANGED)\n\n    # detect the aruco markers in the image\n    # NOTE: do something with the detections here, e.g. publish them to the event service\n    detections, image_vis = detector.detect_pose(image, camera_matrix, distortion_coeff)\n\n    # visualize the image\n    cv2.namedWindow("image", cv2.WINDOW_NORMAL)\n    cv2.imshow("image", image_vis)\n    cv2.waitKey(1)\n')),(0,n.kt)("h3",{id:"5-execute-the-python-script"},"5. Execute the Python script"),(0,n.kt)("p",null,"To run this script from your PC, you need to update the ",(0,n.kt)("inlineCode",{parentName:"p"},"service_config.json"),"\nby modifying the ",(0,n.kt)("inlineCode",{parentName:"p"},"host")," filed with your Amiga brain name (e.g., ",(0,n.kt)("inlineCode",{parentName:"p"},"aluminum-pineapple"),")."),(0,n.kt)("p",null,"You can also stream the stereo left or right images or the camera's disparity by changing\nthe ",(0,n.kt)("inlineCode",{parentName:"p"},"path")," field (e.g., /left) or change the port and service name to instead stream\nthe Oak1 camera."),(0,n.kt)("p",null,"In order to run the example, we need to provide the path to the camera service config file"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-bash"},"python3 main.py --service-config service_config.json\n")),(0,n.kt)("p",null,"Optionally, we can also provide the Aruco dictionary type and the marker size"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-bash"},"python3 main.py --service-config service_config.json --aruco-type DICT_6X6_250 --marker-size 0.1\n")),(0,n.kt)("p",null,"The example will open a window with the camera stream and the detected Aruco markers."),(0,n.kt)("p",null,(0,n.kt)("img",{parentName:"p",src:"https://github.com/farm-ng/amiga-dev-kit/assets/5157099/a0a1abb4-4727-4c2e-be76-b94868dd75fa",alt:"Screenshot from 2023-10-06 16-34-59"})))}d.isMDXComponent=!0}}]);